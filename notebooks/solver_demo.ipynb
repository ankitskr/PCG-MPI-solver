{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2157bd1e-a7f2-44d0-8dee-1e389ab3d744",
   "metadata": {},
   "source": [
    "### Introduction\n",
    "In this demonstration, we show the basic steps needed to perform an elastostatic analysis of a concrete specimen using the PCG-MPI solver."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56cbe685-dada-4e94-8cce-4c8960442412",
   "metadata": {},
   "source": [
    "### Setup environment\n",
    "\n",
    "To utilize the PCG-MPI solver, first install the required libraries and dependencies as outlined in the **`README.md file`** using the specified shell commands. In this project, the environment is set up by exporting the **`notebooks/setup_env.sh`** file as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c0d64d7a-d7fc-45f2-82a7-94409c8d1cd3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-11T15:58:54.762625Z",
     "iopub.status.busy": "2024-07-11T15:58:54.761994Z",
     "iopub.status.idle": "2024-07-11T15:58:54.911625Z",
     "shell.execute_reply": "2024-07-11T15:58:54.910631Z"
    }
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "cat <<EOL > setup_env.sh\n",
    "source ~/env_MPI/bin/activate\n",
    "export PYTHONPATH=~/env_MPI/lib/python3.11/site-packages:\\$PYTHONPATH\n",
    "module load python3/3.11.0\n",
    "module load openmpi/4.0.5\n",
    "\n",
    "export work_dir=~/PCG_MPI_Solver\n",
    "cd \\$work_dir\n",
    "\n",
    "export n_meshparts=8\n",
    "\n",
    "EOL\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cd00028-0440-451b-900e-b7f8690ef35e",
   "metadata": {},
   "source": [
    "### Collect model data\n",
    "We analyze an octree mesh model of a concrete specimen, depicted in Fig. 1. The mesh, generated from a 3D image of a concrete specimen measuring 512x512x512 voxels, was developed by Prof. Chongmin Song's research group at UNSW Sydney. In the mesh, blue cells represent aggregates, while red cells denote mortar. The model data—including the mesh, geometric details, constraints, and elastic material properties—is available in **`data/concrete.zip`**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "05ae77ed-bf90-4360-969e-5e36202710a4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-11T15:58:54.916406Z",
     "iopub.status.busy": "2024-07-11T15:58:54.915604Z",
     "iopub.status.idle": "2024-07-11T15:58:59.233826Z",
     "shell.execute_reply": "2024-07-11T15:58:59.232854Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">creating directories..\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">extracting files from concrete\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">success!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">elements:  124693\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">nodes:     208316 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">dofs:      624948\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">total runtime: 2.06 sec\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "source setup_env.sh\n",
    "\n",
    "export model_name=concrete\n",
    "export scratch_path=$work_dir/data\n",
    "export input_model_path=$scratch_path/$model_name.zip\n",
    "\n",
    "python3 src/data/read_input_model.py $work_dir $model_name $scratch_path $input_model_path\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fe021a7-deba-4de8-87ce-869459a25db1",
   "metadata": {},
   "source": [
    "<div style=\"text-align: left;\">\n",
    "    <img src=\"images/mesh_material.png\" alt=\"Mesh and Material\" width=\"300\"/>\n",
    "    <p>Figure 1: An octree mesh generated from 3D image-based model of a concrete specimen</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bb5edbc-e3f6-461d-8b3b-7a6fe9384511",
   "metadata": {},
   "source": [
    "### Partition mesh\n",
    "\n",
    "The PCG-MPI solver leverages a mesh-partitioning strategy to enhance parallel processing efficiency. In this approach, the mesh model is divided into smaller sections, or mesh partitions. Within each partition, matrix operations are carried out locally on individual MPI processes, each utilizing a physical core. The data among the mesh-partitions is synchronised using inter-process communication.\n",
    "\n",
    "In this demonstration, we configure the mesh partition count to 8 during the environment setup. For large-scale problems, this number can be substantially increased— e.g. 12,000 partitions for models with approximately 1 billion unknowns. Utilizing the Gadi supercomputer at NCI Canberra, which supports over 100,000 cores, enables us to perform massively parallel computations efficiently. To ensure high-quality mesh partitions, we employ the METIS package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cec422f1-0cd5-4468-bcfe-06fcd28fa836",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-11T15:58:59.238613Z",
     "iopub.status.busy": "2024-07-11T15:58:59.237882Z",
     "iopub.status.idle": "2024-07-11T15:59:00.824194Z",
     "shell.execute_reply": "2024-07-11T15:59:00.823119Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">loading model data..\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">generating indices for 8 mesh parts..\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">success!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">total runtime: 0.57 sec\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "source setup_env.sh\n",
    "\n",
    "python3 src/solver/run_metis.py $n_meshparts\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb82e3e1-1527-464f-b163-465e08f0ce85",
   "metadata": {},
   "source": [
    "<div style=\"text-align: left;\">\n",
    "    <img src=\"images/mesh_partition.png\" alt=\"Mesh Partition\" width=\"300\"/>\n",
    "    <p>Figure 2: Mesh partitions shown in different colours</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "360fe634-f1f2-42b2-873a-db909ad5408e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-11T15:59:00.828932Z",
     "iopub.status.busy": "2024-07-11T15:59:00.828262Z",
     "iopub.status.idle": "2024-07-11T15:59:05.303566Z",
     "shell.execute_reply": "2024-07-11T15:59:05.302446Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">loading model data..\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">partitioning mesh into 8 parts using 4 cores..\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">success!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">total runtime: 3.05 sec\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "source setup_env.sh\n",
    "\n",
    "export n_cores=4   \n",
    "mpiexec -np $n_cores --map-by numa python3 src/solver/partition_mesh.py $n_meshparts 0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b01ee046-5cef-43c6-bde5-190aa83e503b",
   "metadata": {},
   "source": [
    "### Run PCG-MPI solver\n",
    "We configure the time-history and solver parameters before running the PCG-MPI solver, as outlined below. This demonstration focuses on solving an elastostatic problem, and it outputs the nodal displacement of the model under specified constraints."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a87bf5ba-f0d2-461e-8066-4bbef20a6c2e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-11T15:59:05.308611Z",
     "iopub.status.busy": "2024-07-11T15:59:05.307777Z",
     "iopub.status.idle": "2024-07-11T15:59:07.597286Z",
     "shell.execute_reply": "2024-07-11T15:59:07.596196Z"
    }
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "source setup_env.sh\n",
    "\n",
    "python3 -c \"\n",
    "from src.utils.file_operations import exportz\n",
    "\n",
    "TimeHistoryParam={'ExportFlag': True, # export results\n",
    "                  'ExportFrmRate' : 1,\n",
    "                  'ExportFrms' : [],\n",
    "                  'PlotFlag' : False,\n",
    "                  'TimeStepDelta': [0,1],\n",
    "                  'ExportVars': 'U'} #D U PS PE GS GE\n",
    "\n",
    "SolverParam={'Tol' : 1e-7,\n",
    "             'MaxIter' : 10000}\n",
    "\n",
    "GlobSettings = {'TimeHistoryParam' : TimeHistoryParam,\n",
    "                'SolverParam' : SolverParam}\n",
    "exportz('__pycache__/GlobSettings.zpkl', GlobSettings)\n",
    "\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a4763182-4605-41dc-8e7b-56beebe3ca5b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-11T15:59:07.602258Z",
     "iopub.status.busy": "2024-07-11T15:59:07.601556Z",
     "iopub.status.idle": "2024-07-11T15:59:22.292730Z",
     "shell.execute_reply": "2024-07-11T15:59:22.291635Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">loading partitioned data..\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">running parallel pcg solver with 8 cores..\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">exporting results..\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">success!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">file read time:     0.2 sec \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">calculation time:   11.5 sec\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">communication time: 1.0 sec\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">---------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">total runtime:      12.6 sec \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        \n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "source setup_env.sh\n",
    "\n",
    "mpiexec -np $n_meshparts --map-by numa python3 src/solver/pcg_solver.py 1 0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40501bcd-6cff-434f-a76b-6d219165617e",
   "metadata": {},
   "source": [
    "As we observe here, the communication time among MPI processes is notably low compared to the calculation time. The mesh-partitioning strategy effectively minimizes communication overhead, maintaining the parallel efficiency even as we scale up to thousands of cores for large-scale problems. Refer to the documents in **`docs/references.txt`** to gain more insights on parallel efficiencies (weak and strong)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8d99fe7-9443-4678-a365-b274a4fd7222",
   "metadata": {},
   "source": [
    "### Export and visualize results\n",
    "The displacement results produced by the solver are exported in a format conducive to model data visualization, such as .vtk. In this work, we use ParaView, which has a client–server architecture to facilitate remote visualization of large-scale datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0bb6cd01-3cda-44d5-ae4b-738a1706aa32",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-11T15:59:22.297606Z",
     "iopub.status.busy": "2024-07-11T15:59:22.296771Z",
     "iopub.status.idle": "2024-07-11T15:59:26.584910Z",
     "shell.execute_reply": "2024-07-11T15:59:26.583847Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">loading model and result data..\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">exporting vtk files..\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">success!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">total runtime: 2.27 sec\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "source setup_env.sh\n",
    "\n",
    "mpiexec -np 1 python3 src/data/export_vtk.py 1 \"U\" \"Full\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b8c9984-f16b-4acc-bcfc-475314e169d4",
   "metadata": {},
   "source": [
    "<img src=\"images/mesh_deformation.png\" alt=\"Mesh Deformation\" width=\"350\"/>\n",
    "Fig.3 Mesh deformation (scaled 1000 times for illustration)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
